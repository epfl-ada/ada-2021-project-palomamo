{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Female citations in UKs leading news papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a first presentation of our project for milestone 2. It is structured and written in such  a way that we can directly continue on it for milestone 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "1. [Setup](#setup)   \n",
    "    1.1 [Imports](#imports)  \n",
    "    1.2 [Data paths](#data_paths)   \n",
    "    1.3 [Utility functions](#utility_functions)   \n",
    "2. [Data preparation](#data_prep)    \n",
    "    2.1 [Columns and rows selection](#cols_rows_select)  \n",
    "    2.2 [News paper selection](#newspaper_select)           \n",
    "    2.3 [Filtering raw data](#filter_raw_data)   \n",
    "    2.4 [Merging speaker information](#merging_speakers)   \n",
    "3. [Data exploration and cleaning](#data_explore_clean)  \n",
    "    3.1 [Import prepared data](#import_prep_data)   \n",
    "    3.2 [Set index](#set_index)    \n",
    "    3.3 [Save cleaned data frame as pickle](#save_pickle)   \n",
    "4. [Research questions](#research_question)     \n",
    "    4.1 [Load pickled dataframes](#load_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports\n",
    "<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import bz2\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data paths\n",
    "<a id=\"Data paths\"></a>\n",
    "\n",
    "**Important**: The raw and prepared data are stored locally in the root folder _raw_data_ and _data_. To excecute section [Data preperation](#data_prep) the raw data is needed. This sections has to be exectuted only once.\n",
    "\n",
    "You can download the raw data here (EPFL google account required): [Quotebank](), [Speakers]()\n",
    "\n",
    "The cleaned data can be found using this link:\n",
    "[Cleaned data]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment the files which aren't locally stored\n",
    "RAW_QUOTES_2020_PATH = 'raw_data/quotes-2020.json.bz2'\n",
    "\n",
    "QUOTES_2020_PATH = 'data/quotes-2020-gb.json.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Utility functions\n",
    "<a id=\"utility_functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mini_version_of_data(path_to_file, chunksize, nb_chunks):\n",
    "    \"\"\"\n",
    "    Returns a mini dataframe from of a bz2 compressed json file.\n",
    "    :path_to_file: file path as string\n",
    "    :chunksize: size to iterate\n",
    "    :nb_chunks: how many chunks\n",
    "    :return: pandas.DataFrame with chunksize*nb_chunks of rows\n",
    "    \"\"\"\n",
    "        \n",
    "    curr_chunk = 0\n",
    "    chunk_list = []\n",
    "    \n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            if curr_chunk == nb_chunks:\n",
    "                break\n",
    "            \n",
    "            curr_chunk = curr_chunk + 1\n",
    "            chunk_list.append(chunk)\n",
    "    \n",
    "    df = pd.concat(chunk_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "<a id=\"data_prep\"></a>\n",
    "\n",
    "The quotebank dataset is too large to directly access it with a dataframe. This section provides all the steps to filter the data we need for our analysis. The filtering and preperation is done based on our research question. Please check the README for details. Further explanations are given under [Research question](#research_question).\n",
    "\n",
    "The data preperation can be done on a per year basis of the Quotebank data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select file of the year you want to prepare\n",
    "path_to_file = RAW_QUOTES_2020_PATH\n",
    "path_to_out = QUOTES_2020_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Column and row selection\n",
    "<a id=\"cols_rows_select\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-28-000082</td>\n",
       "      <td>[ D ] espite the efforts of the partners to cr...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-01-28 08:04:05</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.7272], [Prime Minister Netanyahu, 0....</td>\n",
       "      <td>[http://israelnationalnews.com/News/News.aspx/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-16-000088</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>Sue Myrick</td>\n",
       "      <td>[Q367796]</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Sue Myrick, 0.8867], [None, 0.0992], [Ron Wy...</td>\n",
       "      <td>[http://thehill.com/opinion/international/4782...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-10-000142</td>\n",
       "      <td>... He (Madhav) also disclosed that the illega...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-02-10 23:45:54</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.8926], [Prakash Rai, 0.1074]]</td>\n",
       "      <td>[https://indianexpress.com/article/business/ec...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-15-000053</td>\n",
       "      <td>... [ I ] f it gets to the floor,</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-02-15 14:12:51</td>\n",
       "      <td>2</td>\n",
       "      <td>[[None, 0.581], [Andy Harris, 0.4191]]</td>\n",
       "      <td>[https://patriotpost.us/opinion/68622-trump-bu...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>[ I met them ] when they just turned 4 and 7. ...</td>\n",
       "      <td>Meghan King Edmonds</td>\n",
       "      <td>[Q20684375]</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>4</td>\n",
       "      <td>[[Meghan King Edmonds, 0.5446], [None, 0.2705]...</td>\n",
       "      <td>[https://people.com/parents/meghan-king-edmond...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2020-01-28-000082  [ D ] espite the efforts of the partners to cr...   \n",
       "1  2020-01-16-000088  [ Department of Homeland Security ] was livid ...   \n",
       "2  2020-02-10-000142  ... He (Madhav) also disclosed that the illega...   \n",
       "3  2020-02-15-000053                  ... [ I ] f it gets to the floor,   \n",
       "4  2020-01-24-000168  [ I met them ] when they just turned 4 and 7. ...   \n",
       "\n",
       "               speaker         qids                date  numOccurrences  \\\n",
       "0                 None           [] 2020-01-28 08:04:05               1   \n",
       "1           Sue Myrick    [Q367796] 2020-01-16 12:00:13               1   \n",
       "2                 None           [] 2020-02-10 23:45:54               1   \n",
       "3                 None           [] 2020-02-15 14:12:51               2   \n",
       "4  Meghan King Edmonds  [Q20684375] 2020-01-24 20:37:09               4   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [[None, 0.7272], [Prime Minister Netanyahu, 0....   \n",
       "1  [[Sue Myrick, 0.8867], [None, 0.0992], [Ron Wy...   \n",
       "2            [[None, 0.8926], [Prakash Rai, 0.1074]]   \n",
       "3             [[None, 0.581], [Andy Harris, 0.4191]]   \n",
       "4  [[Meghan King Edmonds, 0.5446], [None, 0.2705]...   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://israelnationalnews.com/News/News.aspx/...     E  \n",
       "1  [http://thehill.com/opinion/international/4782...     E  \n",
       "2  [https://indianexpress.com/article/business/ec...     E  \n",
       "3  [https://patriotpost.us/opinion/68622-trump-bu...     E  \n",
       "4  [https://people.com/parents/meghan-king-edmond...     E  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick look at a small subset of the data of the selected year\n",
    "year_sample_df = load_mini_version_of_data(path_to_file, 10000, 10)\n",
    "year_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34316"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many quotations don't have an assigned speaker?\n",
    "sum(year_sample_df['speaker'] == 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above shows that there around 1/3 of the quotations are 'None' speakers. As we want to make a gender based study will will not need these rows. This eliminitation will drasticly reduce the size of the data we have to analyse.\n",
    "\n",
    "Furthermore the colums which aren't of interest for our study are:\\\n",
    "**phase**: we don't care\\\n",
    "**probas**: as we will select the the speaker with highest probablity (note that 'None' speakers are already neglected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newspaper selection\n",
    "<a id=\"nespaper_select\"></a>\n",
    "In first analysis we will pick quotations of 3 of the top 12 UKs newspapers with the most reach both in prints and digital reach. See [this]() statistic for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>website_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Sun</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>www.theguardian.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Times</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name          website_url\n",
       "0       The Sun                 None\n",
       "1  The Guardian  www.theguardian.com\n",
       "2     The Times                 None"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of selected newspapers and their urls\n",
    "newspapers_list = [['The Sun', ], \n",
    "              ['The Guardian', 'www.theguardian.com'],\n",
    "              ['The Times', ]]\n",
    "\n",
    "# Dataframe\n",
    "newspapers_df = pd.DataFrame(newspapers_list, columns = ['name', 'website_url'])\n",
    "newspapers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filtering raw data\n",
    "<a id=\"filter_raw_data\"></a>\n",
    "\n",
    "Fowolling the reasoning above we can exctract the infos needed from the compressed file of a year of quotations. Let's create a helper function to check the primary urls of a quotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_names(urls):\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        parsed_uri = urlparse(url)\n",
    "        results.append('{uri.netloc}'.format(uri=parsed_uri))\n",
    "    return results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep the rows matching the speficied urls and will drop all rows of 'None' speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_738182/2513117810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mnewspapers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newspapers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewspapers\u001b[0m \u001b[0;31m# updating the sample with domain name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0md_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# writing in the new file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ada/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_can_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mcompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through all instances of json file and extract the desired rows\n",
    "# Save the File in the data directory\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            if instance['speaker'] == None:\n",
    "                break\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            \n",
    "            newspapers = []\n",
    "            website_urls = get_website_names(urls)\n",
    "            for w in website_urls:\n",
    "                for name, website_url in zip(newspapers_df['name'],newspapers_df['website_url']):\n",
    "                    if w == website_url:\n",
    "                        newspapers.append(name)\n",
    "                        instance['newspapers'] = newspapers # updating the sample with domain name\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merge speaker informations\n",
    "<a id=\"merge_speaker_infos\"></a>\n",
    "**Lorena**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data exploration and cleaning\n",
    "<a id=\"data_explore_clean\"></a>\n",
    "**Marie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import prepared data\n",
    "<a id=\"import_prep_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set the index\n",
    "<a id=\"set_index\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The index is now unique\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Save cleaned data frame as pickle\n",
    "<a id=\"save_pickle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Research questions\n",
    "<a id=\"research_questions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load pickled dataframes\n",
    "<a id=\"load_pickle\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
